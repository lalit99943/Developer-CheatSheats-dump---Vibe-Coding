import { CheatSheet } from '../../types';

export const aiCheatSheets: CheatSheet[] = [
  {
    id: 'ai-1',
    category: 'AI',
    subCategory: 'text',
    title: 'AI, ML, DL, NLP Abbreviations',
    snippet: 'AI: Artificial Intelligence\nML: Machine Learning\nDL: Deep Learning\nNLP: Natural Language Processing',
    description: 'Fundamental abbreviations in the AI field. AI is the broad concept of creating intelligent machines. ML is a subset of AI that uses data to train models. DL is a subset of ML using deep neural networks. NLP focuses on computers understanding human language.',
    tags: ['ai', 'ml', 'dl', 'nlp', 'abbreviations', 'concepts'],
  },
  {
    id: 'ai-2',
    category: 'AI',
    subCategory: 'text',
    title: 'LLM and RAG Abbreviations',
    snippet: 'LLM: Large Language Model\nRAG: Retrieval-Augmented Generation',
    description: 'LLMs (e.g., GPT, Gemini) are deep learning models with billions of parameters, trained on vast text data. RAG is a technique that enhances LLM responses by first retrieving relevant information from an external knowledge base and providing it as context.',
    tags: ['ai', 'llm', 'rag', 'language model', 'generation', 'architecture'],
  },
  {
    id: 'ai-3',
    category: 'AI',
    subCategory: 'text',
    title: 'Embeddings Explained',
    snippet: 'Embedding: A numerical vector representation of text, images, or other data, where semantically similar items have similar vectors.',
    description: 'Embeddings are crucial for ML models as they convert high-dimensional, sparse data (like words) into a dense, lower-dimensional space. This allows models to understand relationships and context. For example, the vectors for "king" and "queen" would be closer than "king" and "apple".',
    tags: ['ai', 'embeddings', 'vector', 'nlp', 'representation'],
  },
  {
    id: 'ai-4',
    category: 'AI',
    subCategory: 'text',
    title: 'Vector Databases Explained',
    snippet: 'Vector Database: A database designed to store and query high-dimensional vector embeddings efficiently.',
    description: 'Unlike traditional databases that index text or numbers, vector databases index embeddings. They use algorithms like HNSW or IVF to perform very fast Approximate Nearest Neighbor (ANN) searches, which is essential for tasks like semantic search or recommendation systems.',
    tags: ['ai', 'vector database', 'database', 'ann', 'search', 'embeddings'],
  },
  {
    id: 'ai-5',
    category: 'AI',
    subCategory: 'python',
    title: 'Semantic Search',
    snippet: 'from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\'all-MiniLM-L6-v2\')\nsentences = [\'A man is eating food.\', \'A man is eating a piece of bread.\', \'The girl is playing football.\']\nquery = \'A man is eating a sandwich.\'\n\n# Encode sentences to get embeddings\nsentence_embeddings = model.encode(sentences)\nquery_embedding = model.encode(query)\n\n# Compute cosine similarity\ncos_scores = util.cos_sim(query_embedding, sentence_embeddings)[0]\nprint(f"Similarity with sentence 1: {cos_scores[0]:.2f}") // High\nprint(f"Similarity with sentence 2: {cos_scores[1]:.2f}") // High\nprint(f"Similarity with sentence 3: {cos_scores[2]:.2f}") // Low',
    description: 'Semantic search seeks to understand the intent and contextual meaning of a query, rather than just matching keywords. It works by converting both the query and the documents into embeddings and finding the closest matches in the vector space.',
    tags: ['ai', 'semantic search', 'search', 'nlp', 'embeddings', 'sentence-transformers'],
  },
  {
    id: 'ai-6',
    category: 'AI',
    subCategory: 'text',
    title: 'AI Agents Explained',
    snippet: 'AI Agent: An autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals.',
    description: 'Modern AI agents, often powered by LLMs, can perform complex, multi-step tasks. They can use tools (like APIs or code interpreters), reason about a problem, create a plan, and execute it. The core components are a reasoning engine (LLM), memory, and a set of tools.',
    tags: ['ai', 'agents', 'llm', 'autonomous', 'reasoning', 'tools'],
  },
  {
    id: 'ai-7',
    category: 'AI',
    subCategory: 'text',
    title: 'Model Context Explained',
    snippet: 'Context / Context Window: The amount of text (input + output) a language model can process at one time.',
    description: 'The context window is a key limitation of LLMs, measured in tokens. All information provided in a prompt (instructions, examples, retrieved documents) must fit within this window. If the context is exceeded, the model loses information from the beginning of the conversation. Providing good, relevant context is the core of effective prompting.',
    tags: ['ai', 'llm', 'context', 'context window', 'prompting', 'tokens'],
  },
  {
    id: 'ai-8',
    category: 'AI',
    subCategory: 'python',
    title: 'NumPy: N-dimensional Arrays',
    snippet: 'import numpy as np\n\n# Create a 1D array from a list\na = np.array([1, 2, 3])\n\n# Create a 2D array (matrix)\nb = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Perform element-wise operations\nc = a + 5 // [6, 7, 8]\n\nprint(b.shape) // (2, 3)',
    description: 'NumPy is a fundamental library for scientific computing in Python. Its core feature is the powerful N-dimensional array object, which allows for efficient mathematical operations on large datasets.',
    tags: ['ai', 'python', 'numpy', 'arrays', 'data science', 'library'],
  },
  {
    id: 'ai-9',
    category: 'AI',
    subCategory: 'python',
    title: 'Pandas: DataFrame Basics',
    snippet: 'import pandas as pd\n\ndata = {\'Name\': [\'Alice\', \'Bob\', \'Charlie\'],\n        \'Age\': [25, 30, 35]}\n\ndf = pd.DataFrame(data)\n\n# Select a column\nages = df[\'Age\']\n\n# Add a new column\ndf[\'City\'] = [\'New York\', \'Paris\', \'London\']\n\nprint(df.head())',
    description: 'Pandas is the primary library for data manipulation and analysis in Python. The DataFrame is its main data structure, representing a 2D labeled table with columns of potentially different types.',
    tags: ['ai', 'python', 'pandas', 'dataframe', 'data analysis', 'library'],
  },
  {
    id: 'ai-10',
    category: 'AI',
    subCategory: 'python',
    title: 'Scikit-learn: Train/Test Split',
    snippet: 'from sklearn.model_selection import train_test_split\n\n# X are your features, y is your target variable\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)',
    description: 'A crucial step in model evaluation. The data is split into a training set (used to train the model) and a testing set (used to evaluate the trained model\'s performance on unseen data). This helps to check for overfitting.',
    tags: ['ai', 'scikit-learn', 'ml', 'train test split', 'evaluation'],
  },
  {
    id: 'ai-11',
    category: 'AI',
    subCategory: 'python',
    title: 'Scikit-learn: Training a Model',
    snippet: 'from sklearn.linear_model import LogisticRegression\n\n# 1. Instantiate the model\nmodel = LogisticRegression()\n\n# 2. Train the model using the training data\nmodel.fit(X_train, y_train)\n\n# 3. Make predictions on new data\npredictions = model.predict(X_test)',
    description: 'The standard Scikit-learn workflow involves creating an instance of a model, training it with the `.fit()` method, and using it to make predictions with the `.predict()` method.',
    tags: ['ai', 'scikit-learn', 'ml', 'model', 'fit', 'predict', 'training'],
  },
  {
    id: 'ai-12',
    category: 'AI',
    subCategory: 'python',
    title: 'Scikit-learn: Model Evaluation',
    snippet: 'from sklearn.metrics import accuracy_score, classification_report\n\n# Get predictions from the model\npredictions = model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f"Accuracy: {accuracy:.2f}")\n\n# Get a detailed report (precision, recall, f1-score)\nprint(classification_report(y_test, predictions))',
    description: 'After training, a model\'s performance must be evaluated on the test set. Common metrics for classification include accuracy, precision, recall, and the F1-score.',
    tags: ['ai', 'scikit-learn', 'ml', 'metrics', 'evaluation', 'accuracy', 'precision', 'recall'],
  },
  {
    id: 'ai-13',
    category: 'AI',
    subCategory: 'python',
    title: 'PyTorch: Basic Neural Network',
    snippet: 'import torch\nimport torch.nn as nn\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.layer1 = nn.Linear(in_features=784, out_features=128)\n        self.layer2 = nn.Linear(in_features=128, out_features=10)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = self.layer2(x)\n        return x\n\nmodel = SimpleNet()',
    description: 'PyTorch is a popular deep learning framework. Neural networks are defined by creating a class that inherits from `nn.Module`, defining the layers in `__init__`, and specifying the forward pass logic in the `forward` method.',
    tags: ['ai', 'pytorch', 'deep learning', 'neural network', 'dl', 'library'],
  },
  {
    id: 'ai-14',
    category: 'AI',
    subCategory: 'python',
    title: 'TensorFlow/Keras: Sequential Model',
    snippet: 'import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(128, activation=\'relu\', input_shape=(784,)),\n    Dense(10, activation=\'softmax\')\n])\n\nmodel.compile(optimizer=\'adam\', \n              loss=\'sparse_categorical_crossentropy\', \n              metrics=[\'accuracy\'])',
    description: 'TensorFlow is another major deep learning framework. Keras is its high-level API, which makes building models straightforward. The Sequential model is a simple stack of layers.',
    tags: ['ai', 'tensorflow', 'keras', 'deep learning', 'neural network', 'dl', 'library'],
  },
  {
    id: 'ai-15',
    category: 'AI',
    subCategory: 'python',
    title: 'Hugging Face: Using a Pipeline',
    snippet: 'from transformers import pipeline\n\n# Create a sentiment analysis pipeline\nclassifier = pipeline(\'sentiment-analysis\')\n\n# Use the pipeline to classify text\nresult = classifier(\'I love using the Hugging Face library!\')\n\nprint(result) # [{\'label\': \'POSITIVE\', \'score\': 0.99...}]',
    description: 'The Hugging Face Transformers library simplifies using pre-trained models for various NLP tasks. The `pipeline` function is the easiest way to use a model for inference on a specific task like sentiment analysis or translation.',
    tags: ['ai', 'hugging face', 'transformers', 'nlp', 'pipeline', 'pre-trained'],
  },
  {
    id: 'ai-16',
    category: 'AI',
    subCategory: 'python',
    title: 'Data Preprocessing: Feature Scaling',
    snippet: 'from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit on training data and transform it\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Only transform the test data (using the scaler fitted on train data)\nX_test_scaled = scaler.transform(X_test)',
    description: 'Many ML algorithms perform better when numerical input features are scaled to a standard range. StandardScaler standardizes features by removing the mean and scaling to unit variance.',
    tags: ['ai', 'ml', 'preprocessing', 'scaling', 'scikit-learn'],
  },
  {
    id: 'ai-17',
    category: 'AI',
    subCategory: 'text',
    title: 'Overfitting and Underfitting',
    snippet: 'Overfitting: Model performs great on training data but poorly on unseen test data. It has learned the training data "too well", including its noise.\nUnderfitting: Model performs poorly on both training and test data. It has failed to capture the underlying patterns in the data.',
    description: 'The central challenge in machine learning is finding the balance between overfitting and underfitting. Techniques like cross-validation, regularization, and using more data can combat overfitting.',
    tags: ['ai', 'ml', 'overfitting', 'underfitting', 'model tuning', 'concepts'],
  },
  {
    id: 'ai-18',
    category: 'AI',
    subCategory: 'text',
    title: 'Fine-Tuning a Model',
    snippet: 'Fine-Tuning: The process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt it to a new task.',
    description: 'Instead of training a large model from scratch, it\'s often more effective to fine-tune an existing one. This leverages the general knowledge learned by the pre-trained model and specializes it for your particular use case, saving significant time and resources.',
    tags: ['ai', 'fine-tuning', 'transfer learning', 'llm', 'pre-trained', 'nlp'],
  },
  {
    id: 'ai-19',
    category: 'AI',
    subCategory: 'text',
    title: 'Activation Functions',
    snippet: 'ReLU (Rectified Linear Unit): Outputs the input directly if positive, otherwise outputs zero. Common choice for hidden layers.\nSigmoid: Squeezes numbers into the range (0, 1). Useful for binary classification output layers.\nSoftmax: Converts a vector of numbers into a probability distribution. Useful for multi-class classification output layers.',
    description: 'In a neural network, the activation function of a node defines the output of that node given an input or set of inputs. It introduces non-linearity into the model, allowing it to learn complex patterns.',
    tags: ['ai', 'dl', 'neural network', 'activation function', 'relu', 'sigmoid', 'softmax'],
  },
];
